{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9490b36f-e3b7-49db-8271-befda3ac7891",
   "metadata": {},
   "source": [
    "Lungs Cancer ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc94dbd-8f4a-4510-aecc-12a655a6624d",
   "metadata": {},
   "source": [
    "Problem Satement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144c9e7-342b-4483-9622-d47c5258f9a5",
   "metadata": {},
   "source": [
    "About Data Sets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247652c-1f43-457c-9161-4983e6fa3246",
   "metadata": {},
   "source": [
    "Aims and Objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90550c78-1160-4d89-8f86-738116a6e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14d9ef-6792-41e2-bc5c-34795c90e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "#Sklearn -libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "#To balance datasets \n",
    "from sklearn.utils import class_weight\n",
    "#Open CV\n",
    "import cv2\n",
    "#Tensorflow and keras Library\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tesnorflow.keras.applications import VGG16\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136b17c-8e5b-4678-bce9-ff342dbd5561",
   "metadata": {},
   "outputs": [],
   "source": [
    "bengin_path = r'./NIDCH-Dhaka-Lungs-Cancer-Datasets/Dataset/Bengin cases'\n",
    "malignant_path = r'./NIDCH-Dhaka-Lungs-Cancer-Datasets/Dataset/Malignant cases'\n",
    "normal_path = r'./NIDCH-Dhaka-Lungs-Cancer-Datasets/Dataset/Normal cases'\n",
    "\n",
    "img_classes = [\"Bengin cases\", \"Malignant cases\", \"Normal cases\"] # Categories \n",
    "path_list = [bengin_path, malignant_path, normal_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529b3a9-d2ab-430c-bee7-d2b9d0430446",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = []\n",
    "class_labels = []\n",
    "for i, dir_list in enumerate(path_list):\n",
    "    name_img = os.listdir(dir_list)\n",
    "    for name_file in name_img:\n",
    "        img = os.path.join(dir_list,name_file)\n",
    "        img_path.append(img)\n",
    "        class_labels.append(img_classes[i])\n",
    "\n",
    "df = pd.DataFrame({\"img_path\" : img_path,\n",
    "                  \"label\" : class_labels})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89fb66-5f03-497e-9f86-4fe41683b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f7259-d6fb-462b-bd47-ed0ecbae78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category, group in df.groupby(\"label\"):\n",
    "    fig, ax = plt.subplots(1,3, figsize = (7,7))\n",
    "    ax = ax.ravel()\n",
    "    for i, (_,r) in enumerate(group.sample(3).iterrows()):\n",
    "        img = cv2.imread(r.img_path)\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].axis(\"off\")\n",
    "        ax[i].set_title(r.label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4725ad4-1cf9-4ab3-b469-8f083222d34d",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38fc5ad-5647-42b7-b523-d505fbe068e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "countData = df[\"label\"].value_counts().reset_index()\n",
    "fig = px.histogram(data_frame = countData, x = \"label\", y = \"count\", width=600,\n",
    "    height=400,\n",
    "    title=\"Count of Labels by Category\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705897ec-f1f1-442c-af73-7f91b5b9593d",
   "metadata": {},
   "source": [
    "#### Analyzing the Image properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7673f2-5ec0-4a12-8303-ceb1e0bd9d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "resolutions = []\n",
    "color_distributions = []\n",
    "\n",
    "for img_path in df [\"img_path\"]:\n",
    "    # load image \n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #code to get image size\n",
    "    size = os.path.getsize(img_path)\n",
    "    sizes.append(size)\n",
    "    #Extract the resolutions of images\n",
    "    resolution = img.shape[:2]\n",
    "    resolutions.append(resolution)\n",
    "    #Extract color distribution\n",
    "    mean_color_distributions = np.bincount(img.flatten(), minlength= 256)\n",
    "    color_distributions.append(mean_color_distributions)\n",
    "sizes = np.array(sizes)\n",
    "resolutions = np.array(resolutions)\n",
    "color_distributions = np.array(color_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a259714-bfaa-4824-8216-846240d52824",
   "metadata": {},
   "source": [
    "#### Distributions Size of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb3cf9-f9f8-4f8d-9547-7dc8c3349e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image sizes in MB\n",
    "sizes_MB = []\n",
    "for img_path in df[\"img_path\"]:\n",
    "    #code to load image\n",
    "    img = cv2.imread(img_path)\n",
    "    #code to get imge size \n",
    "    size = os.path.getsize(img_path)\n",
    "    sizes_MB.append(size/1_00_000)\n",
    "\n",
    "fig = px.histogram(x=sizes_MB, nbins = 50, title = \"Distribution size of Image\", width=600,\n",
    "    height=400)\n",
    "fig.update_layout(xaxis_title = \"File Size (MB)\",\n",
    "                  yaxis_title = \"Number of Images\",\n",
    "                  showlegend = False,\n",
    "                  bargap = 0.1,\n",
    "                  bargroupgap = 0.1)\n",
    "fig.update_traces(marker = dict(color=\"green\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1580ab-ed19-4e2b-b69d-6c168171abb3",
   "metadata": {},
   "source": [
    "#### Distributions of image Resulations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf1ee9-fb1c-4c95-85df-b67c9d915bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x = resolutions[:,0],\n",
    "                 y = resolutions[:,1], \n",
    "                 title = \"Distribution of Image Resolution\", height= 400, width= 600)\n",
    "fig.update_layout(\n",
    "    xaxis_title = \"Width (Pixel)\",\n",
    "    yaxis_title = \"Height (Pixel)\",\n",
    "    showlegend = False,\n",
    "    hovermode = \"closest\"\n",
    ")\n",
    "fig.update_traces(marker = dict(color=\"red\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb6289-f772-4c42-9346-9a2c80ac4b68",
   "metadata": {},
   "source": [
    "### Mean Color Distributins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba301bc9-730d-4976-95f3-8c25760ec900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "mean_color_distributions = np.mean(color_distributions, axis = 0)\n",
    "fig = go.Figure(\n",
    "    go.Bar(x = np.arange(256), y = mean_color_distributions, name = \"Mean Color Distributions\"\n",
    "))\n",
    "fig.update_layout(\n",
    "    title = \"Mean Color Distribution\",\n",
    "    xaxis_title = \"Color Values\",\n",
    "    yaxis_title = \"Number of Pixel\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd002e-5c7d-4ec7-bb16-635d8a048d05",
   "metadata": {},
   "source": [
    "#### Train & test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50460c27-f970-4d25-97e5-abd65d445917",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.70\n",
    "test_ratio = 0.15\n",
    "val_ratio = 0.15\n",
    "\n",
    "df_train, df_test_val = train_test_split(df, train_size = train_ratio, random_state = 42)\n",
    "df_test, df_val = train_test_split(df_test_val, train_size = test_ratio/(test_ratio + val_ratio), random_state = 42)\n",
    "\n",
    "print(f\"Train shape = {df_train.shape}\")\n",
    "print(f\"Test shape = {df_test.shape}\")\n",
    "print(f\"Validation shape = {df_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b6b85-b4e9-4229-adb0-2e2bed085486",
   "metadata": {},
   "source": [
    "#### DeNoise image using median blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b3d21-68e7-4ca6-a74f-1093b3d6af98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_denoise(img):\n",
    "    denoise_img = cv2.medianBlur(img, 1)\n",
    "    denoise_img = cv2.cvtColor(denoise_img, cv2.COLOR_BGR2RGB)\n",
    "    return denoise_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6994be-e0c5-4a4c-9528-72620074aa43",
   "metadata": {},
   "source": [
    "#### Data Agrumendations using ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc166f0-49c9-442a-999b-162fb91ebc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "image_size = (IMG_WIDTH, IMG_HEIGHT)\n",
    "batch_size = 32\n",
    "\n",
    "TRAIN_DATAGEN = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   preprocessing_function = preprocessing_denoise,\n",
    "                                  rotation_range = 15,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  shear_range = 0.1,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True,\n",
    "                                  )\n",
    "\n",
    "\n",
    "TEST_DATAGEN = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "#for training image \n",
    "train_generator = TRAIN_DATAGEN.flow_from_dataframe(\n",
    "    df_train,\n",
    "    x_col = \"img_path\",\n",
    "    y_col = \"label\",\n",
    "    traget_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb', \n",
    "    class_mode = \"categorical\",\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "# for testing image \n",
    "test_generator = TEST_DATAGEN.flow_from_dataframe(\n",
    "    df_test,\n",
    "    x_col = \"img_path\",\n",
    "    y_col = \"label\",\n",
    "    traget_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb', \n",
    "    class_mode = \"categorical\",\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "val_generator = TEST_DATAGEN.flow_from_dataframe(\n",
    "    df_val,\n",
    "    x_col = \"img_path\",\n",
    "    y_col = \"label\",\n",
    "    traget_size = image_size,\n",
    "    batch_size = batch_size,\n",
    "    color_mode = 'rgb', \n",
    "    class_mode = \"categorical\",\n",
    "    shuffle = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87616d0-8805-403e-a390-73247f916271",
   "metadata": {},
   "source": [
    "#### Class weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755943d0-2a42-4dbb-93d8-0f0485c53a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The datasets is hightly imblace so we used Class weight for balancing the datasets \n",
    "\n",
    "#image categories\n",
    "classes = list(train_generator.class_indices.keys())\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "           class_weight = 'balanced',\n",
    "            classes = np.unique(train_generator.classes),\n",
    "            y = train_generator.classes)\n",
    "train_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "#classes = list(train_generator.class_indices.keys())\n",
    "for idx, weight, in train_class_weights.items():\n",
    "    class_name = classes[idx]\n",
    "    print(f\"{class_name} : {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe49f8-ea03-409d-a289-b2ca353f9a43",
   "metadata": {},
   "source": [
    "#### Model  Implementations #3D-CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40701046-caf0-4766-b33a-d8b20f9ba392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_2D = Sequential([\n",
    "    # Conv Block 1\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(256,256, 3)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv Block 2\n",
    "    Conv2D(64, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv Block 3\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv Block 4\n",
    "    Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv Block 5\n",
    "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Conv Block 6\n",
    "    Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Global Average Pooling\n",
    "    GlobalAveragePooling2D(),\n",
    "\n",
    "    # Fully Connected Layers\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Dense(3, activation='softmax')  # Assuming 3 classes\n",
    "]) \n",
    "\n",
    "model_2D.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f0185-2eb5-4378-a8e6-84f2f18df130",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2D.compile(\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    optimizer = Adam(learning_rate = 0.0003),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "epochs = 50\n",
    "history = model_2D.fit(train_generator,\n",
    "                   steps_per_epoch = len(train_generator),\n",
    "                   batch_size = 32,\n",
    "                   validation_data = val_generator,\n",
    "                   validation_steps = len(val_generator),\n",
    "                   class_weight = train_class_weights,\n",
    "                   callbacks=[\n",
    "                               EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True),\n",
    "                               ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, mode='min')\n",
    "                              ],\n",
    "                   epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65373df5-8b4d-49b4-a339-393ce778b09f",
   "metadata": {},
   "source": [
    "##### Training & validation Acuracy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e2192-f612-46db-84f6-6d9e962ca460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(epochs, history):\n",
    "    fig1 = make_subplots()\n",
    "    fig1.add_trace(go.Scatter(x = np.arange(1,epochs+1), y = history.history[\"accuracy\"], name = \"Training Accuracy\"))\n",
    "    fig1.add_trace(go.Scatter(x = np.arange(1,epochs+1), y = history.history[\"val_accuracy\"], name = \"Validation Accuracy\"))\n",
    "    fig1.update_layout(title = \"Training and Validation Accuracy\", xaxis_title = \"Epoch\", yaxis_title = \"Accuracy\")\n",
    "    fig1.show()\n",
    "\n",
    "    fig2 = make_subplots()\n",
    "    fig2.add_trace(go.Scatter(x = np.arange(1,epochs+1), y = history.history[\"loss\"], name = \"Training Loss\"))\n",
    "    fig2.add_trace(go.Scatter(x = np.arange(1,epochs+1), y = history.history[\"val_loss\"], name = \"Validation Loss\"))\n",
    "    fig2.update_layout(title = \"Training and Validation Loss\", xaxis_title = \"Epoch\", yaxis_title = \"Loss\")\n",
    "    fig2.show()\n",
    "\n",
    "history_plot(epochs, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57328b49-3628-499b-90d4-3e4f47de324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_generator):\n",
    "    #Calculate test loss and accuracy\n",
    "    results = model.evaluate(test_generator, verbose = 0)\n",
    "    print(f\"Test Loss = {results[0]}\")\n",
    "    print(f\"Test Accuracy = {results[1]}\")\n",
    "evaluate_model(model_2D, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a7a4b-dd9d-4892-b6f3-611686a1adab",
   "metadata": {},
   "source": [
    "#### Transfer learning for improving Accuray (using Fine tune Model Vgg16 and InceptionsV3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169b8e9-b887-4bc5-b2eb-05d14f92de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define base_model of InceptionV3\n",
    "base_model = InceptionV3(input_shape = (256,256,3), include_top = False, weights = \"imagenet\")\n",
    "#Freeze all layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Add Custom layers\n",
    "model_IV3 = Sequential()\n",
    "model_IV3.add(Input(shape = (256,256, 3)))\n",
    "model_IV3.add(base_model)\n",
    "model_IV3.add(GlobalAveragePooling2D())\n",
    "model_IV3.add(Dense(1024, activation = \"relu\"))\n",
    "model_IV3.add(Dropout(0.4))\n",
    "model_IV3.add(Dense(3, activation = \"softmax\"))\n",
    "#Compile and Training the model \n",
    "epochs = 50\n",
    "model_IV3.compile(optimizer = Adam(0.0002),\n",
    "             loss = \"categorical_crossentropy\",\n",
    "             metrics = [\"accuracy\"])\n",
    "history = model_IV3.fit(train_generator,\n",
    "                   steps_per_epoch = len(train_generator),\n",
    "                   batch_size = 64,\n",
    "                   validation_data = val_generator,\n",
    "                   validation_steps = len(val_generator),\n",
    "                   class_weight = train_class_weights,\n",
    "                   callbacks=[\n",
    "                               EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True), \n",
    "                               ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min') \n",
    "                              ],\n",
    "                   epochs = epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcb4f0-eb4f-48a5-868f-ec8748c5e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the graph\n",
    "history_plot(epochs, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5aeea-2ffe-405d-87bc-4a5080e808ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_IV3, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4521069-deff-4232-8b23-6489cdd97767",
   "metadata": {},
   "source": [
    "### Fine tune InceptionsV3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8800db4-abce-48c7-b209-06398485c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All layers does not freeze\n",
    "base_model.trainable = True\n",
    "#Compile the model\n",
    "model_IV3.compile(optimizer = Adam(0.0001),\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"])\n",
    "#Training the model\n",
    "epochs = 50\n",
    "history = model_IV3.fit(train_generator,\n",
    "                   steps_per_epoch = len(train_generator),\n",
    "                   batch_size = 64,\n",
    "                   validation_data = val_generator,\n",
    "                   validation_steps = len(val_generator),\n",
    "                   class_weight = train_class_weights,\n",
    "                   callbacks=[\n",
    "                               EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True), \n",
    "                               ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min') \n",
    "                              ],\n",
    "                   epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d255009-180a-43ae-8ac1-949e1f8ee70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the graph\n",
    "history_plot(epochs, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9bdb3-7834-40f7-b28f-716453e59f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_IV3, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6926e-4b71-44a4-9d17-313daeb0547d",
   "metadata": {},
   "source": [
    "### Finetune VGG16 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d0138-6433-4022-b3e6-8bba261841ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define base mode of VGG16\n",
    "base_model_vgg16 = VGG16(input_shape = (256,256, 3), include_top = False, weights = \"imagenet\")\n",
    "\n",
    "#Freeze all layers VGG16 model\n",
    "for layer in base_model_vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Add custom layers\n",
    "model_VGG16 = Sequential()\n",
    "model_VGG16 .add(Input(shape = (256, 256, 3)))\n",
    "model_VGG16 .add(base_model)\n",
    "model_VGG16 .add(GlobalAveragePooling2D())\n",
    "model_VGG16 .add(Dense(1024, activation = \"relu\"))\n",
    "model_VGG16 .add(Dropout(0.4))\n",
    "model_VGG16 .add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "#Compile model\n",
    "model_VGG16 .compile(optimizer = Adam(0.0002),\n",
    "             loss = \"categorical_crossentropy\",\n",
    "             metrics = [\"accuracy\"])\n",
    "\n",
    "#Training the model\n",
    "epochs = 50\n",
    "history = model_VGG16 .fit(train_generator,\n",
    "                   steps_per_epoch = len(train_generator),\n",
    "                   batch_size = 64,\n",
    "                   validation_data = val_generator,\n",
    "                   validation_steps = len(val_generator),\n",
    "                   class_weight = train_class_weights,\n",
    "                   callbacks=[\n",
    "                               EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True), \n",
    "                               ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, mode='min') \n",
    "                              ],\n",
    "                   epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a6388-5555-47e6-ad8a-e68b55966c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(epochs, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee116f9-1ed1-4bf8-8ee7-4a0cb3dc4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_VGG16 ,test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45717c48-22f1-4629-95f2-7982572866dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the training layer jus block_conv1\n",
    "base_model_vgg16.trainable = True\n",
    "set_trainable = False\n",
    "for layer in base_model_vgg16.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "#Compile the model\n",
    "model_VGG16 .compile(optimizer = Adam(0.0001),\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"])   \n",
    "#Training the model\n",
    "epochs = 50\n",
    "history = model_VGG16.fit(train_generator,\n",
    "                   steps_per_epoch = len(train_generator),\n",
    "                   batch_size = 64,\n",
    "                   validation_data = val_generator,\n",
    "                   validation_steps = len(val_generator),\n",
    "                   class_weight = train_class_weights,\n",
    "                   callbacks=[\n",
    "                               EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
    "                               patience = 5,\n",
    "                               restore_best_weights = True), \n",
    "                               ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, mode='min') \n",
    "                              ],\n",
    "                   epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a2ec0-7d88-4d33-bbf4-cfbbed72f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(epochs,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b9e48-0626-4811-8077-6d89dee7df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_VGG16, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e149a6-411c-4eed-acc4-3dda2fda5628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results of the model\n",
    "models = [model_2D, model_IV3, model_VGG16 ]\n",
    "loss = []\n",
    "accuracy = []\n",
    "for model in models:\n",
    "    results = model.evaluate(test_generator, verbose = 0)\n",
    "    loss.append(results[0])\n",
    "    accuracy.append(results[1])\n",
    "\n",
    "name_models = [\"CNN (Custom)\", \"InceptionV3\", \"VGG16\"]\n",
    "df_loss_acc = pd.DataFrame(data = {\"Name_Models\" : name_models,\n",
    "                                  \"Loss\" : loss,\n",
    "                                  \"Accuracy\" : accuracy})\n",
    "\n",
    "fig = px.bar(data_frame = df_loss_acc, x = \"Name_Models\", y = [\"Accuracy\", \"Loss\"],\n",
    "            barmode = \"group\",\n",
    "            text_auto = \".3f\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
